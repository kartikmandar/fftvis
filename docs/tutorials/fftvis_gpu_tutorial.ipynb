{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7221ef1",
   "metadata": {},
   "source": [
    "# Running `fftvis` with the GPU Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce8dc6",
   "metadata": {},
   "source": [
    "In this notebook, we'll demonstrate how to use `fftvis` with GPU acceleration to simulate visibilities. The GPU backend can significantly accelerate simulations, especially for large catalogs of point sources or extended observations with many frequencies and time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0836a520",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__Note__\n",
    "\n",
    "This tutorial requires a CUDA-capable GPU and the `cupy` package installed. If you don't have a compatible GPU, you can use the CPU backend instead.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d021b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "__Warning__\n",
    "\n",
    "Before running this tutorial, make sure you have installed `fftvis` with GPU support. You can check if your GPU is properly configured by running a simple test with `cupy`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd48302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "try:\n",
    "    import cupy as cp\n",
    "    print(f\"CuPy version: {cp.__version__}\")\n",
    "    print(f\"CUDA version: {cp.cuda.runtime.runtimeGetVersion()}\")\n",
    "    print(f\"Available GPU(s): {cp.cuda.runtime.getDeviceCount()}\")\n",
    "    for i in range(cp.cuda.runtime.getDeviceCount()):\n",
    "        dev = cp.cuda.Device(i)\n",
    "        props = cp.cuda.runtime.getDeviceProperties(i)\n",
    "        print(f\"  Device {i}: {props['name'].decode()}, Memory: {props['totalGlobalMem']/1e9:.2f} GB\")\n",
    "    gpu_available = True\n",
    "except ImportError:\n",
    "    print(\"CuPy is not installed. Please install it to use the GPU backend.\")\n",
    "    gpu_available = False\n",
    "except cp.cuda.runtime.CUDARuntimeError:\n",
    "    print(\"CUDA runtime error: No CUDA-capable device is detected or CUDA driver is not installed properly.\")\n",
    "    gpu_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac710755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "from astropy.time import Time\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# HERA-stack imports\n",
    "import fftvis\n",
    "from hera_sim.antpos import hex_array\n",
    "from pyuvdata.telescopes import Telescope\n",
    "from pyuvdata.analytic_beam import AiryBeam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e479a59",
   "metadata": {},
   "source": [
    "## Setup Telescope / Observation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cef897",
   "metadata": {},
   "source": [
    "We'll set up the same observation parameters as in the CPU backend tutorial for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9baa4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define antenna array positions\n",
    "antpos = hex_array(3, split_core=True, outriggers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15234c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define antenna beam using pyuvdata.analytic_beam.AiryBeam with a dish size of 14 meters\n",
    "beam = AiryBeam(diameter=14.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a06a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of frequencies in units of Hz\n",
    "nfreqs = 20\n",
    "freqs = np.linspace(100e6, 120e6, nfreqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of times with an astropy time.Time object\n",
    "ntimes = 30\n",
    "times = Time(np.linspace(2459845, 2459845.05, ntimes), format='jd', scale='utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd734be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the telescope location\n",
    "telescope_loc = Telescope.from_known_telescopes('hera').location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0540d40",
   "metadata": {},
   "source": [
    "## Setup Sky Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866079fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a sky model using HEALPix\n",
    "nside = 64\n",
    "nsource = hp.nside2npix(nside)\n",
    "\n",
    "# Get HEALPix pixel coordinates\n",
    "dec, ra = hp.pix2ang(nside, np.arange(nsource))\n",
    "dec -= np.pi / 2  # Convert from co-latitude to declination\n",
    "\n",
    "# Define the flux of the sources as a function of frequency\n",
    "flux = np.random.uniform(0, 1, nsource)                # flux of each source at 100MHz (in Jy)\n",
    "alpha = np.ones(nsource) * -0.8                        # spectral index of each source\n",
    "\n",
    "# Now get the (Nsource, Nfreq) array of the flux of each source at each frequency\n",
    "flux_allfreq = ((freqs[:, np.newaxis] / freqs[0]) ** alpha.T * flux.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45d711",
   "metadata": {},
   "source": [
    "## Run `fftvis` with GPU Backend\n",
    "\n",
    "Unlike the CPU backend which uses `finufft` for non-uniform FFT operations, the GPU backend in `fftvis` uses `cufinufft` through CuPy. This allows for significant speedups, especially with large numbers of sources. \n",
    "\n",
    "To use the GPU backend, we simply specify `backend=\"gpu\"` in the `simulate_vis` function call. All other parameters remain the same as in the CPU version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd21e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subset of baselines we're interested in for simulating\n",
    "baselines = [(i, j) for i in range(len(antpos)) for j in range(len(antpos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb9b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Simulate visibilities using the GPU backend\n",
    "if gpu_available:\n",
    "    vis_gpu = fftvis.simulate_vis(\n",
    "        ants=antpos,\n",
    "        fluxes=flux_allfreq,\n",
    "        ra=ra,\n",
    "        dec=dec,\n",
    "        freqs=freqs,\n",
    "        times=times.jd,\n",
    "        telescope_loc=telescope_loc,\n",
    "        beam=beam,\n",
    "        polarized=False,\n",
    "        precision=2,\n",
    "        nprocesses=1,  # Use single process for GPU simulation\n",
    "        baselines=baselines,\n",
    "        backend=\"gpu\"  # Use GPU backend\n",
    "    )\n",
    "else:\n",
    "    print(\"GPU not available. Skipping GPU simulation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bfea0",
   "metadata": {},
   "source": [
    "## Compare GPU and CPU Backends\n",
    "\n",
    "Let's run the same simulation with the CPU backend for comparison. We expect the results to be nearly identical, with the main difference being the computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da580ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Simulate visibilities using the CPU backend\n",
    "vis_cpu = fftvis.simulate_vis(\n",
    "    ants=antpos,\n",
    "    fluxes=flux_allfreq,\n",
    "    ra=ra,\n",
    "    dec=dec,\n",
    "    freqs=freqs,\n",
    "    times=times.jd,\n",
    "    telescope_loc=telescope_loc,\n",
    "    beam=beam,\n",
    "    polarized=False,\n",
    "    precision=2,\n",
    "    nprocesses=1,\n",
    "    baselines=baselines,\n",
    "    backend=\"cpu\"  # Use CPU backend\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that results from GPU and CPU are equivalent\n",
    "if gpu_available:\n",
    "    # The results should be very close but not exactly the same due to floating-point differences\n",
    "    print(f\"Maximum absolute difference: {np.max(np.abs(vis_gpu - vis_cpu))}\")\n",
    "    print(f\"Are GPU and CPU results close? {np.allclose(vis_gpu, vis_cpu, rtol=1e-5, atol=1e-7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aeae35",
   "metadata": {},
   "source": [
    "## Benchmark Performance: GPU vs CPU\n",
    "\n",
    "Let's benchmark the performance difference between GPU and CPU backends with increasing number of sources. The GPU advantage typically becomes more apparent with larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4841d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performance(nsides, ntimes=10, nfreqs=5):\n",
    "    \"\"\"Benchmark GPU vs CPU performance for different HEALPix nsides.\"\"\"\n",
    "    \n",
    "    # Shorter time and frequency arrays for benchmarking\n",
    "    short_freqs = np.linspace(100e6, 120e6, nfreqs)\n",
    "    short_times = Time(np.linspace(2459845, 2459845.02, ntimes), format='jd', scale='utc')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for nside in nsides:\n",
    "        nsource = hp.nside2npix(nside)\n",
    "        print(f\"Running benchmark with nside={nside}, nsource={nsource}\")\n",
    "        \n",
    "        # Create sky model\n",
    "        dec, ra = hp.pix2ang(nside, np.arange(nsource))\n",
    "        dec -= np.pi / 2\n",
    "        flux = np.random.uniform(0, 1, nsource)\n",
    "        alpha = np.ones(nsource) * -0.8\n",
    "        flux_allfreq = ((short_freqs[:, np.newaxis] / short_freqs[0]) ** alpha.T * flux.T).T\n",
    "        \n",
    "        # Time CPU simulation\n",
    "        t0 = time.time()\n",
    "        _ = fftvis.simulate_vis(\n",
    "            ants=antpos,\n",
    "            fluxes=flux_allfreq,\n",
    "            ra=ra,\n",
    "            dec=dec,\n",
    "            freqs=short_freqs,\n",
    "            times=short_times.jd,\n",
    "            telescope_loc=telescope_loc,\n",
    "            beam=beam,\n",
    "            polarized=False,\n",
    "            precision=2,\n",
    "            nprocesses=1,\n",
    "            baselines=baselines[:10],  # Use fewer baselines for speed\n",
    "            backend=\"cpu\"\n",
    "        )\n",
    "        cpu_time = time.time() - t0\n",
    "        \n",
    "        # Time GPU simulation\n",
    "        gpu_time = None\n",
    "        if gpu_available:\n",
    "            t0 = time.time()\n",
    "            _ = fftvis.simulate_vis(\n",
    "                ants=antpos,\n",
    "                fluxes=flux_allfreq,\n",
    "                ra=ra,\n",
    "                dec=dec,\n",
    "                freqs=short_freqs,\n",
    "                times=short_times.jd,\n",
    "                telescope_loc=telescope_loc,\n",
    "                beam=beam,\n",
    "                polarized=False,\n",
    "                precision=2,\n",
    "                nprocesses=1,\n",
    "                baselines=baselines[:10],  # Use fewer baselines for speed\n",
    "                backend=\"gpu\"\n",
    "            )\n",
    "            gpu_time = time.time() - t0\n",
    "            # Clear GPU memory\n",
    "            if 'cp' in globals():\n",
    "                cp.cuda.runtime.deviceSynchronize()\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "        \n",
    "        results.append((nside, nsource, cpu_time, gpu_time))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks with increasing HEALPix nside values\n",
    "# Skip if GPU is not available\n",
    "if gpu_available:\n",
    "    benchmark_results = benchmark_performance([8, 16, 32, 64])\n",
    "else:\n",
    "    print(\"GPU not available. Skipping benchmarks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fff76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the benchmark results\n",
    "if gpu_available and 'benchmark_results' in locals():\n",
    "    nsides, nsources, cpu_times, gpu_times = zip(*benchmark_results)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot execution times\n",
    "    ax1.plot(nsources, cpu_times, 'o-', label='CPU')\n",
    "    ax1.plot(nsources, gpu_times, 's-', label='GPU')\n",
    "    ax1.set_xlabel('Number of Sources')\n",
    "    ax1.set_ylabel('Execution Time (s)')\n",
    "    ax1.set_title('Execution Time Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot speedup ratios\n",
    "    speedups = [cpu/gpu for cpu, gpu in zip(cpu_times, gpu_times)]\n",
    "    ax2.plot(nsources, speedups, 'o-')\n",
    "    ax2.set_xlabel('Number of Sources')\n",
    "    ax2.set_ylabel('Speedup (CPU time / GPU time)')\n",
    "    ax2.set_title('GPU Speedup Factor')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff08726",
   "metadata": {},
   "source": [
    "## Plot Visibility Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52804f31",
   "metadata": {},
   "source": [
    "We'll plot the visibility amplitude and phase from the GPU simulation, similar to what we did in the CPU tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU results if available, otherwise CPU results\n",
    "vis_to_plot = vis_gpu if gpu_available else vis_cpu\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 6))\n",
    "for bl_index, bl in enumerate(baselines[:3]):\n",
    "    axs[0].semilogy(freqs / 1e6, np.abs(vis_to_plot[:, 0, bl_index]))\n",
    "    axs[1].plot(freqs / 1e6, np.angle(vis_to_plot[:, 0, bl_index]), label=f\"b = {bl[0]}\")\n",
    "\n",
    "axs[1].legend()\n",
    "axs[0].set_xlabel('Frequency [MHz]')\n",
    "axs[1].set_xlabel('Frequency [MHz]')\n",
    "axs[0].set_ylabel('Amplitude [Jy]')\n",
    "axs[1].set_ylabel('Phase [rad]')\n",
    "axs[1].set_ylim(-np.pi * 1.1, np.pi * 1.1)\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 6))\n",
    "for bl_index, bl in enumerate(baselines[:3]):\n",
    "    axs[0].semilogy(times.unix - times.unix[0], np.abs(vis_to_plot[0, :, bl_index]))\n",
    "    axs[1].plot(times.unix - times.unix[0], np.angle(vis_to_plot[0, :, bl_index]), label=f\"b = {bl[0]}\")\n",
    "\n",
    "axs[0].set_xlabel('Times [s]')\n",
    "axs[1].set_xlabel('Times [s]')\n",
    "axs[0].set_ylabel('Amplitude [Jy]')\n",
    "axs[1].set_ylabel('Phase [rad]')\n",
    "axs[1].set_ylim(-np.pi * 1.1, np.pi * 1.1)\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b18612",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The GPU backend of `fftvis` provides a significant speedup compared to the CPU backend, especially for larger sky models with many sources. The main advantages are:\n",
    "\n",
    "1. Accelerated non-uniform FFT operations using `cufinufft`\n",
    "2. Parallel processing of source computations on the GPU\n",
    "3. Efficient beam interpolation using GPU-accelerated map coordinates\n",
    "\n",
    "When working with large simulations, the GPU backend is recommended if suitable hardware is available. For smaller simulations, the overhead of data transfer between CPU and GPU might reduce the performance advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory if we used it\n",
    "if gpu_available and 'cp' in globals():\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "    cp.get_default_memory_pool().free_all_blocks()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
