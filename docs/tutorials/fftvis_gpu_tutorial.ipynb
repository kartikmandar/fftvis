{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7221ef1",
   "metadata": {},
   "source": [
    "# Running `fftvis` with the GPU Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce8dc6",
   "metadata": {},
   "source": [
    "In this notebook, we'll give a brief demonstration of how to use `fftvis` with GPU acceleration to simulate visibilities for a set of input simulation parameters. This tutorial is very similar to the one included in the `matvis` simulator package. One should consolunt that tutorial for a difference in the use cases of `fftvis` and `matvis`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0836a520",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__Note__\n",
    "\n",
    "The absolute easiest way to use `fftvis` is via the `hera_sim` [plugin interface](https://hera-sim.readthedocs.io/en/latest/tutorials/hera_sim_vis_cli.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d021b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "__Warning__\n",
    "\n",
    "Before running this tutorial, you should make sure you understand the basic concepts and algorithm that `fftvis` uses. You can read up on that here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd48302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartikmandar/anaconda3/envs/matvis-env/lib/python3.11/site-packages/pyuvdata/analytic_beam.py:111: UserWarning: basis_vector_type was not defined, defaulting to azimuth and zenith_angle.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "from astropy.time import Time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# HERA-stack imports\n",
    "import fftvis\n",
    "import matvis\n",
    "from hera_sim.antpos import hex_array\n",
    "from pyuvdata.telescopes import Telescope\n",
    "from pyuvdata.analytic_beam import AiryBeam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e479a59",
   "metadata": {},
   "source": [
    "## Setup Telescope / Observation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cef897",
   "metadata": {},
   "source": [
    "We need a few input parameters to setup our observation: antenna positions, beam models, and a sky model.\n",
    "\n",
    "Here we will set up a very simple observation for introductory purposes.\n",
    "\n",
    "First, create our antenna positions. We define this as a dictionary, which maps an antenna number to its 3D East-North-Up position relative to the array centre. Here, we define just a hexagonal array of 15 antennas using `hera_sim.antpos.hex_array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9baa4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define antenna array positions\n",
    "antpos = hex_array(3, split_core=True, outriggers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beam_description",
   "metadata": {},
   "source": [
    "Next, we define the beam to be used by all antennas in the array. Unlike `matvis` and `pyuvsim`, `fftvis` currently restricts users to a single beam for all antennas. The specified beam must be a `UVBeam` or `AnalyticBeam` object from `pyuvdata`. Alternatively, you can create a custom `AnalyticBeam` class (see the pyuvdata tutorial on `UVBeam` objects for guidance). For this simulation, we will use a simple, frequency-dependent Airy beam corresponding to a dish size of 14 meters.\n",
    "\n",
    "**Note for GPU backend**: The GPU backend requires UVBeam objects for beam evaluation. We'll convert our AnalyticBeam to a UVBeam for GPU compatibility while maintaining the same beam parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15234c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define antenna beam using pyuvdata.analytic_beam.AiryBeam with a dish size of 14 meters\n",
    "analytic_beam = AiryBeam(diameter=14.0)\n",
    "\n",
    "# For GPU compatibility, convert AnalyticBeam to UVBeam\n",
    "# We'll define the frequency range and angular resolution for the conversion\n",
    "freq_array = np.linspace(100e6, 120e6, 20)  # Same as our observation frequencies\n",
    "naz, nza = 360, 180  # Angular resolution\n",
    "beam = analytic_beam.to_uvbeam(\n",
    "    freq_array=freq_array,\n",
    "    axis1_array=np.linspace(0, 2 * np.pi, naz + 1)[:-1],  # azimuth\n",
    "    axis2_array=np.linspace(0, np.pi, nza + 1),  # zenith angle\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obs_config_description",
   "metadata": {},
   "source": [
    "We also required to provide `fftvis` with the observational configuration including a frequency array, a time array, and a telescope location. The frequency array specifies the observation frequencies in units of Hz. The time array defines the observation times using an `astropy.time.Time` object, with times specified in Julian Dates and configured with the appropriate format and scale. The telescope location specifies the geographic position of the array and can be defined either using `astropy.coordinates.EarthLocation` with a known site name or through `pyuvdata.telescopes.Telescope` by selecting a predefined telescope location supported within `pyuvdata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0a06a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of frequencies in units of Hz\n",
    "nfreqs = 20\n",
    "freqs = np.linspace(100e6, 120e6, nfreqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42b9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of times with an astropy time.Time object\n",
    "ntimes = 30\n",
    "times = Time(np.linspace(2459845, 2459845.05, ntimes), format='jd', scale='utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcd734be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import EarthLocation\n",
    "\n",
    "# define using astropy.coordinates.EarthLocation\n",
    "telescope_loc = EarthLocation.of_site('meerkat')\n",
    "\n",
    "# define the telescope location using the pyuvdata.telescopes.Telescope\n",
    "telescope_loc = Telescope.from_known_telescopes('hera').location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0540d40",
   "metadata": {},
   "source": [
    "## Setup Sky Model\n",
    "\n",
    "Like `matvis`, `fftvis` makes the point source approximation -- that is it makes breaks a continuous sky model into a discrete number of point sources that it sums over when computing the visibilities. In this notebook, we'll assume the point source approximation by discretizing the sky with a randomly generated HEALpix map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "866079fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sources\n",
    "nside = 64\n",
    "nsource = hp.nside2npix(nside)\n",
    "\n",
    "# pixels can be defined as point sources randomly distributed over the full sky\n",
    "ra = np.deg2rad(np.random.uniform(0, 360, nsource))        # ra of each source (in rad)\n",
    "dec = np.deg2rad(np.random.uniform(-90, 90.0, nsource))    # dec of each source (in rad)\n",
    "\n",
    "# define sky model using healpix map\n",
    "dec, ra = hp.pix2ang(nside, np.arange(nsource))\n",
    "dec -= np.pi / 2\n",
    "\n",
    "# define the flux of the sources as a function of frequency. Here, we define smooth spectrum sources\n",
    "flux = np.random.uniform(0, 1, nsource)                              # flux of each source at 100MHz (in Jy)\n",
    "alpha = np.ones(nsource) * -0.8                      # sp. index of each source\n",
    "\n",
    "# Now get the (Nsource, Nfreq) array of the flux of each source at each frequency.\n",
    "flux_allfreq = ((freqs[:, np.newaxis] / freqs[0]) ** alpha.T * flux.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45d711",
   "metadata": {},
   "source": [
    "## Run `fftvis` in single processor mode\n",
    "\n",
    "Now that we've setup all our parameters, we can easily run the simulation using the high-level wrapper API. Along with the configuration we've already defined, the `fftvis.simulate.simulate_vis` wrapper takes a few extra options. One of the most important is `polarized`: if true, then full polarized visibilities are returned (with shape `(nfreqs, ntimes, nfeed, nfeed, nbls)`), otherwise, unpolarized visibilities are returned (with shape (nfreqs, ntimes, nants, nants)). In our case, the `AnalyticBeam` objects we're using don't support polarization, so we set this to false.\n",
    "\n",
    "We can also set the `precision` parameter, which switches between 32-bit (if `precision=1`) and 64-bit (if `precision=2`) floating precision. In addition to controlling the floating point precision, the `fftvis` also includes a parameter `eps` which can improve the precision of the underlying Non-Uniform FFT. The runtime of the simulation is roughly inversely proportional to the `eps` parameter for a set `precision` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acd21e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subset of baselines we're interested in for simulating\n",
    "baselines = [(i, j) for i in range(len(antpos)) for j in range(len(antpos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bfb9b0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (24576,) (24576, 20) (24576,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m~/fftvis/src/fftvis/wrapper.py:216\u001b[0m, in \u001b[0;36msimulate_vis\u001b[0;34m(ants, fluxes, ra, dec, freqs, times, beam, telescope_loc, baselines, precision, polarized, eps, beam_spline_opts, use_feed, flat_array_tol, interpolation_function, nprocesses, nthreads, coord_method, coord_method_params, force_use_ray, trace_mem, backend)\u001b[0m\n\u001b[1;32m    213\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_simulation_engine(backend\u001b[38;5;241m=\u001b[39mbackend)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Run the simulation\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfluxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfluxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mra\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtelescope_loc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtelescope_loc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolarized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolarized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_spline_opts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_spline_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_array_tol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_array_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolation_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnprocesses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnprocesses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoord_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoord_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoord_method_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoord_method_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_use_ray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_use_ray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrace_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrace_mem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fftvis/src/fftvis/gpu/gpu_simulate.py:286\u001b[0m, in \u001b[0;36mGPUSimulationEngine.simulate\u001b[0;34m(self, ants, freqs, fluxes, beam, ra, dec, times, telescope_loc, baselines, precision, polarized, eps, beam_spline_opts, flat_array_tol, interpolation_function, nprocesses, nthreads, coord_method, coord_method_params, force_use_ray, trace_mem, enable_memory_monitor)\u001b[0m\n\u001b[1;32m    280\u001b[0m nthreads_per_proc \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    282\u001b[0m ] \u001b[38;5;241m*\u001b[39m nprocesses  \u001b[38;5;66;03m# Each GPU process typically uses 1 thread for NUFFT\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nthi, fc, tc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nthreads_per_proc, freq_chunks, time_chunks):\n\u001b[1;32m    285\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 286\u001b[0m         \u001b[43mfnc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfreq_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbeam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass Ray object ref or direct object\u001b[39;49;00m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoord_mgr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoord_mgr_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass Ray object ref or direct object\u001b[39;49;00m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrotation_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotation_matrix_gpu_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass Ray object ref or direct object\u001b[39;49;00m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbls_gpu_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass Ray object ref or direct object\u001b[39;49;00m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfreqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_gpu_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass Ray object ref or direct object\u001b[39;49;00m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcomplex_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnfeeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnfeeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpolarized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolarized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m            \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbeam_spline_opts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_spline_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m            \u001b[49m\u001b[43minterpolation_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnthi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ignored by GPU NUFFT\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_coplanar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_coplanar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrace_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrace_mem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# --- Retrieve Results ---\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_ray:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# Ray returns futures, get the results\u001b[39;00m\n",
      "File \u001b[0;32m~/fftvis/src/fftvis/gpu/gpu_simulate.py:477\u001b[0m, in \u001b[0;36mGPUSimulationEngine._evaluate_vis_chunk\u001b[0;34m(self, time_idx, freq_idx, beam, coord_mgr, rotation_matrix, bls, freqs, complex_dtype, nfeeds, polarized, eps, beam_spline_opts, interpolation_function, n_threads, is_coplanar, trace_mem)\u001b[0m\n\u001b[1;32m    474\u001b[0m     A_s \u001b[38;5;241m=\u001b[39m A_s\u001b[38;5;241m.\u001b[39mreshape(nfeeds \u001b[38;5;241m*\u001b[39m nfeeds, nsim_sources)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# Multiply by flux_sqrt\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m     \u001b[43mA_s\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mflux_sqrt\u001b[49m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# A_s is now the apparent flux, shape (nsim_sources,)\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Ensure weights have correct complex dtype\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A_s\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m complex_dtype:\n",
      "File \u001b[0;32mcupy/_core/core.pyx:1446\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__imul__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/_kernel.pyx:1314\u001b[0m, in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/internal.pyx:381\u001b[0m, in \u001b[0;36mcupy._core.internal._broadcast_core\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (24576,) (24576, 20) (24576,)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# simulate visibilities with the new API\n",
    "vis_vc = fftvis.simulate_vis(\n",
    "    ants=antpos,\n",
    "    fluxes=flux_allfreq,\n",
    "    ra=ra,\n",
    "    dec=dec,\n",
    "    freqs=freqs,\n",
    "    times=times.jd,\n",
    "    telescope_loc=telescope_loc,\n",
    "    beam=beam,\n",
    "    polarized=False,\n",
    "    precision=2,\n",
    "    nprocesses=1,\n",
    "    baselines=baselines,\n",
    "    backend=\"gpu\"  # Use GPU backend instead of CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bfea0",
   "metadata": {},
   "source": [
    "## Compare GPU and CPU Backends\n",
    "\n",
    "Let's run the same simulation with the CPU backend for comparison. We expect the results to be nearly identical, with the main difference being the computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da580ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Simulate visibilities using the CPU backend\n",
    "vis_cpu = fftvis.simulate_vis(\n",
    "    ants=antpos,\n",
    "    fluxes=flux_allfreq,\n",
    "    ra=ra,\n",
    "    dec=dec,\n",
    "    freqs=freqs,\n",
    "    times=times.jd,\n",
    "    telescope_loc=telescope_loc,\n",
    "    beam=beam,\n",
    "    polarized=False,\n",
    "    precision=2,\n",
    "    nprocesses=1,\n",
    "    baselines=baselines,\n",
    "    backend=\"cpu\"  # Use CPU backend\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that results from GPU and CPU are equivalent\n",
    "if gpu_available:\n",
    "    # The results should be very close but not exactly the same due to floating-point differences\n",
    "    print(f\"Maximum absolute difference: {np.max(np.abs(vis_gpu - vis_cpu))}\")\n",
    "    print(f\"Are GPU and CPU results close? {np.allclose(vis_gpu, vis_cpu, rtol=1e-5, atol=1e-7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aeae35",
   "metadata": {},
   "source": [
    "## Benchmark Performance: GPU vs CPU\n",
    "\n",
    "Let's benchmark the performance difference between GPU and CPU backends with increasing number of sources. The GPU advantage typically becomes more apparent with larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4841d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performance(nsides, ntimes=10, nfreqs=5):\n",
    "    \"\"\"Benchmark GPU vs CPU performance for different HEALPix nsides.\"\"\"\n",
    "    \n",
    "    # Shorter time and frequency arrays for benchmarking\n",
    "    short_freqs = np.linspace(100e6, 120e6, nfreqs)\n",
    "    short_times = Time(np.linspace(2459845, 2459845.02, ntimes), format='jd', scale='utc')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for nside in nsides:\n",
    "        nsource = hp.nside2npix(nside)\n",
    "        print(f\"Running benchmark with nside={nside}, nsource={nsource}\")\n",
    "        \n",
    "        # Create sky model\n",
    "        dec, ra = hp.pix2ang(nside, np.arange(nsource))\n",
    "        dec -= np.pi / 2\n",
    "        flux = np.random.uniform(0, 1, nsource)\n",
    "        alpha = np.ones(nsource) * -0.8\n",
    "        flux_allfreq = ((short_freqs[:, np.newaxis] / short_freqs[0]) ** alpha.T * flux.T).T\n",
    "        \n",
    "        # Time CPU simulation\n",
    "        t0 = time.time()\n",
    "        _ = fftvis.simulate_vis(\n",
    "            ants=antpos,\n",
    "            fluxes=flux_allfreq,\n",
    "            ra=ra,\n",
    "            dec=dec,\n",
    "            freqs=short_freqs,\n",
    "            times=short_times.jd,\n",
    "            telescope_loc=telescope_loc,\n",
    "            beam=beam,\n",
    "            polarized=False,\n",
    "            precision=2,\n",
    "            nprocesses=1,\n",
    "            baselines=baselines[:10],  # Use fewer baselines for speed\n",
    "            backend=\"cpu\"\n",
    "        )\n",
    "        cpu_time = time.time() - t0\n",
    "        \n",
    "        # Time GPU simulation\n",
    "        gpu_time = None\n",
    "        if gpu_available:\n",
    "            t0 = time.time()\n",
    "            _ = fftvis.simulate_vis(\n",
    "                ants=antpos,\n",
    "                fluxes=flux_allfreq,\n",
    "                ra=ra,\n",
    "                dec=dec,\n",
    "                freqs=short_freqs,\n",
    "                times=short_times.jd,\n",
    "                telescope_loc=telescope_loc,\n",
    "                beam=beam,\n",
    "                polarized=False,\n",
    "                precision=2,\n",
    "                nprocesses=1,\n",
    "                baselines=baselines[:10],  # Use fewer baselines for speed\n",
    "                backend=\"gpu\"\n",
    "            )\n",
    "            gpu_time = time.time() - t0\n",
    "            # Clear GPU memory\n",
    "            if 'cp' in globals():\n",
    "                cp.cuda.runtime.deviceSynchronize()\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "        \n",
    "        results.append((nside, nsource, cpu_time, gpu_time))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks with increasing HEALPix nside values\n",
    "# Skip if GPU is not available\n",
    "if gpu_available:\n",
    "    benchmark_results = benchmark_performance([8, 16, 32, 64])\n",
    "else:\n",
    "    print(\"GPU not available. Skipping benchmarks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fff76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the benchmark results\n",
    "if gpu_available and 'benchmark_results' in locals():\n",
    "    nsides, nsources, cpu_times, gpu_times = zip(*benchmark_results)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot execution times\n",
    "    ax1.plot(nsources, cpu_times, 'o-', label='CPU')\n",
    "    ax1.plot(nsources, gpu_times, 's-', label='GPU')\n",
    "    ax1.set_xlabel('Number of Sources')\n",
    "    ax1.set_ylabel('Execution Time (s)')\n",
    "    ax1.set_title('Execution Time Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot speedup ratios\n",
    "    speedups = [cpu/gpu for cpu, gpu in zip(cpu_times, gpu_times)]\n",
    "    ax2.plot(nsources, speedups, 'o-')\n",
    "    ax2.set_xlabel('Number of Sources')\n",
    "    ax2.set_ylabel('Speedup (CPU time / GPU time)')\n",
    "    ax2.set_title('GPU Speedup Factor')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff08726",
   "metadata": {},
   "source": [
    "## Plot Visibility Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52804f31",
   "metadata": {},
   "source": [
    "We'll plot the visibility amplitude and phase from the GPU simulation, similar to what we did in the CPU tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU results if available, otherwise CPU results\n",
    "vis_to_plot = vis_gpu if gpu_available else vis_cpu\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 6))\n",
    "for bl_index, bl in enumerate(baselines[:3]):\n",
    "    axs[0].semilogy(freqs / 1e6, np.abs(vis_to_plot[:, 0, bl_index]))\n",
    "    axs[1].plot(freqs / 1e6, np.angle(vis_to_plot[:, 0, bl_index]), label=f\"b = {bl[0]}\")\n",
    "\n",
    "axs[1].legend()\n",
    "axs[0].set_xlabel('Frequency [MHz]')\n",
    "axs[1].set_xlabel('Frequency [MHz]')\n",
    "axs[0].set_ylabel('Amplitude [Jy]')\n",
    "axs[1].set_ylabel('Phase [rad]')\n",
    "axs[1].set_ylim(-np.pi * 1.1, np.pi * 1.1)\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 6))\n",
    "for bl_index, bl in enumerate(baselines[:3]):\n",
    "    axs[0].semilogy(times.unix - times.unix[0], np.abs(vis_to_plot[0, :, bl_index]))\n",
    "    axs[1].plot(times.unix - times.unix[0], np.angle(vis_to_plot[0, :, bl_index]), label=f\"b = {bl[0]}\")\n",
    "\n",
    "axs[0].set_xlabel('Times [s]')\n",
    "axs[1].set_xlabel('Times [s]')\n",
    "axs[0].set_ylabel('Amplitude [Jy]')\n",
    "axs[1].set_ylabel('Phase [rad]')\n",
    "axs[1].set_ylim(-np.pi * 1.1, np.pi * 1.1)\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b18612",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The GPU backend of `fftvis` provides a significant speedup compared to the CPU backend, especially for larger sky models with many sources. The main advantages are:\n",
    "\n",
    "1. Accelerated non-uniform FFT operations using `cufinufft`\n",
    "2. Parallel processing of source computations on the GPU\n",
    "3. Efficient beam interpolation using GPU-accelerated map coordinates\n",
    "\n",
    "When working with large simulations, the GPU backend is recommended if suitable hardware is available. For smaller simulations, the overhead of data transfer between CPU and GPU might reduce the performance advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory if we used it\n",
    "if gpu_available and 'cp' in globals():\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "    cp.get_default_memory_pool().free_all_blocks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
